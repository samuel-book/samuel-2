{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating synthetic SAMUeL data  data with SMOTE\n",
    "\n",
    "## Description of SMOTE\n",
    "\n",
    "SMOTE stands for Synthetic Minority Oversampling Technique [1]. SMOTE is more commonly used to create additional data to enhance modelling fitting, especially when one or more classes have low prevalence in the data set. Hence the description of *oversampling*. \n",
    "\n",
    "SMOTE works by finding near-neighbor points in the original data, and creating new data points from interpolating between two near-neighbor points.\n",
    "\n",
    "Here we remove the real data used to create the synthetic data, leaving only the synthetic data. After generating synthetic data we remove any data points that, by chance, are identical to original real data points, and also remove 10% of points that are closest to the original data points. We measure 'closeness' by the Cartesian distance between standardised data values.\n",
    "\n",
    "![](./images/smote.png)\n",
    "\n",
    "*Demonstration of SMOTE method. (a) Data points with two features (shown on x and y axes) are represented. Points are colour-coded by class label. (b) A data point from a class is picked at random, shown by the black point, and then the closest neighbours of the same class are identified, as shown by yellow points. Here we show 3 closest neighbours, but the default in the SMOTE `Imbalanced-Learn` library is 6. One of those near-neighbour points is selected at random (shown by the second black point). A new data point, shown in red, is created at a random distance between the two selected data points.*\n",
    "\n",
    "### Handling integer, binary, and categorical data\n",
    "\n",
    "The standard SMOTE method generates floating point non-integer) values between data points. There are alternative ways of handing integer, binary, and categorical data using the SMOTE method. Here the methods we use are:\n",
    "\n",
    "* *Integer* values: Round the resulting synthetic data point value to the closest integer.\n",
    "\n",
    "* *Binary*: Code the value as 0 or 1, and round the resulting synthetic data point value to the closest integer (0 or 1).\n",
    "\n",
    "* *Categorical*: One-hot encode the categorical feature. Generate the synthetic data for each category value. Identify the category with the highest value and set to 1 while setting all others to 0.\n",
    "\n",
    "### Implementation with IMBLEARN\n",
    "\n",
    "Here use the implementation in the IMBLEARN IMBALANCED-LEARN [2] \n",
    "\n",
    "[1] Chawla, N.V., Bowyer, K.W., Hall, L.O., Kegelmeyer, W.P. “SMOTE: Synthetic minority over-sampling technique,” Journal of Artificial Intelligence Research, vol. 16, pp. 321-357, 2002.\n",
    "\n",
    "[2] Lemaitre, G., Nogueira, F. and Aridas, C. (2016), Imbalanced-learn: A Python Toolbox to Tackle the Curse of Imbalanced Datasets in Machine Learning. arXiv:1609.06570 (https://pypi.org/project/imbalanced-learn/, `pip install imbalanced-learn`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Import machine learning methods\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# Import package for SMOTE\n",
    "import imblearn\n",
    "\n",
    "# Turn warnings off to keep notebook clean\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loc = './../data/sam_1/kfold_5fold/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data, k_fold_synthetic_data = [], [], []\n",
    "\n",
    "for i in range(5):\n",
    "    \n",
    "    train_data.append(pd.read_csv(data_loc + 'train_{0}.csv'.format(i)))\n",
    "    test_data.append(pd.read_csv(data_loc + 'test_{0}.csv'.format(i)))\n",
    "    k_fold_synthetic_data.append(pd.read_csv(data_loc + 'synth_train_{0}.csv'.format(i)))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to standardise data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardise_data(X_train, X_test):\n",
    "    \"\"\"\n",
    "    Converts all data to a similar scale.\n",
    "    Standardisation subtracts mean and divides by standard deviation\n",
    "    for each feature.\n",
    "    Standardised data will have a mena of 0 and standard deviation of 1.\n",
    "    The training data mean and standard deviation is used to standardise both\n",
    "    training and test set data.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialise a new scaling object for normalising input data\n",
    "    sc = StandardScaler() \n",
    "\n",
    "    # Set up the scaler just on the training set\n",
    "    sc.fit(X_train)\n",
    "\n",
    "    # Apply the scaler to the training and test sets\n",
    "    train_std=sc.transform(X_train)\n",
    "    test_std=sc.transform(X_test)\n",
    "    \n",
    "    return train_std, test_std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to calculate accuracy measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(observed, predicted):\n",
    "    \n",
    "    \"\"\"\n",
    "    Calculates a range of accuracy scores from observed and predicted classes.\n",
    "    \n",
    "    Takes two list or NumPy arrays (observed class values, and predicted class \n",
    "    values), and returns a dictionary of results.\n",
    "    \n",
    "     1) observed positive rate: proportion of observed cases that are +ve\n",
    "     2) Predicted positive rate: proportion of predicted cases that are +ve\n",
    "     3) observed negative rate: proportion of observed cases that are -ve\n",
    "     4) Predicted negative rate: proportion of predicted cases that are -ve  \n",
    "     5) accuracy: proportion of predicted results that are correct    \n",
    "     6) precision: proportion of predicted +ve that are correct\n",
    "     7) recall: proportion of true +ve correctly identified\n",
    "     8) f1: harmonic mean of precision and recall\n",
    "     9) sensitivity: Same as recall\n",
    "    10) specificity: Proportion of true -ve identified:        \n",
    "    11) positive likelihood: increased probability of true +ve if test +ve\n",
    "    12) negative likelihood: reduced probability of true +ve if test -ve\n",
    "    13) false positive rate: proportion of false +ves in true -ve patients\n",
    "    14) false negative rate: proportion of false -ves in true +ve patients\n",
    "    15) true positive rate: Same as recall\n",
    "    16) true negative rate\n",
    "    17) positive predictive value: chance of true +ve if test +ve\n",
    "    18) negative predictive value: chance of true -ve if test -ve\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Converts list to NumPy arrays\n",
    "    if type(observed) == list:\n",
    "        observed = np.array(observed)\n",
    "    if type(predicted) == list:\n",
    "        predicted = np.array(predicted)\n",
    "    \n",
    "    # Calculate accuracy scores\n",
    "    observed_positives = observed == 1\n",
    "    observed_negatives = observed == 0\n",
    "    predicted_positives = predicted == 1\n",
    "    predicted_negatives = predicted == 0\n",
    "    \n",
    "    true_positives = (predicted_positives == 1) & (observed_positives == 1)\n",
    "    \n",
    "    false_positives = (predicted_positives == 1) & (observed_positives == 0)\n",
    "    \n",
    "    true_negatives = (predicted_negatives == 1) & (observed_negatives == 1)\n",
    "    \n",
    "    false_negatives = (predicted_negatives == 1) & (observed_negatives == 0)\n",
    "    \n",
    "    accuracy = np.mean(predicted == observed)\n",
    "    \n",
    "    precision = (np.sum(true_positives) /\n",
    "                 (np.sum(true_positives) + np.sum(false_positives)))\n",
    "        \n",
    "    recall = np.sum(true_positives) / np.sum(observed_positives)\n",
    "    \n",
    "    sensitivity = recall\n",
    "    \n",
    "    f1 = 2 * ((precision * recall) / (precision + recall))\n",
    "    \n",
    "    specificity = np.sum(true_negatives) / np.sum(observed_negatives)\n",
    "    \n",
    "    positive_likelihood = sensitivity / (1 - specificity)\n",
    "    \n",
    "    negative_likelihood = (1 - sensitivity) / specificity\n",
    "    \n",
    "    false_positive_rate = 1 - specificity\n",
    "    \n",
    "    false_negative_rate = 1 - sensitivity\n",
    "    \n",
    "    true_positive_rate = sensitivity\n",
    "    \n",
    "    true_negative_rate = specificity\n",
    "    \n",
    "    positive_predictive_value = (np.sum(true_positives) / \n",
    "                                 np.sum(observed_positives))\n",
    "    \n",
    "    negative_predictive_value = (np.sum(true_negatives) / \n",
    "                                  np.sum(observed_negatives))\n",
    "    \n",
    "    # Create dictionary for results, and add results\n",
    "    results = dict()\n",
    "    \n",
    "    results['observed_positive_rate'] = np.mean(observed_positives)\n",
    "    results['observed_negative_rate'] = np.mean(observed_negatives)\n",
    "    results['predicted_positive_rate'] = np.mean(predicted_positives)\n",
    "    results['predicted_negative_rate'] = np.mean(predicted_negatives)\n",
    "    results['accuracy'] = accuracy\n",
    "    results['precision'] = precision\n",
    "    results['recall'] = recall\n",
    "    results['f1'] = f1\n",
    "    results['sensitivity'] = sensitivity\n",
    "    results['specificity'] = specificity\n",
    "    results['positive_likelihood'] = positive_likelihood\n",
    "    results['negative_likelihood'] = negative_likelihood\n",
    "    results['false_positive_rate'] = false_positive_rate\n",
    "    results['false_negative_rate'] = false_negative_rate\n",
    "    results['true_positive_rate'] = true_positive_rate\n",
    "    results['true_negative_rate'] = true_negative_rate\n",
    "    results['positive_predictive_value'] = positive_predictive_value\n",
    "    results['negative_predictive_value'] = negative_predictive_value\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to create raw synthetic data\n",
    "\n",
    "Data generated are floats; this will need processing for integer, binary, and categorical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_synthetic_data_smote(X, y, number_of_samples=[1000,1000]):\n",
    "    \"\"\"\n",
    "    Synthetic data generation for two classes.\n",
    "        \n",
    "    Inputs\n",
    "    ------\n",
    "    original_data: X, y numpy arrays (y should have label 0 and 1)\n",
    "    number_of_samples: number of samples to generate (list for y=0, y=1)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    X_synthetic: NumPy array\n",
    "    y_synthetic: NumPy array\n",
    "    \"\"\"\n",
    "    \n",
    "    from imblearn.over_sampling import SMOTE\n",
    "    \n",
    "    # Count instances in each class\n",
    "    count_label_0 = np.sum(y==0)\n",
    "    count_label_1 = np.sum(y==1)\n",
    "    \n",
    "    # SMOTE requires final class counts; add current counts to required counts\n",
    "    n_class_0 = number_of_samples[0] + count_label_0\n",
    "    n_class_1 = number_of_samples[1] + count_label_1\n",
    "\n",
    "    # Get SMOTE points\n",
    "    X_resampled, y_resampled = SMOTE(\n",
    "        sampling_strategy = {0:n_class_0, 1:n_class_1}).fit_resample(X, y)\n",
    "\n",
    "    # Get just the additional (synethetic) data points\n",
    "    X_synthetic = X_resampled[len(X):]\n",
    "    y_synthetic = y_resampled[len(y):]\n",
    "                                                                   \n",
    "    return X_synthetic, y_synthetic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to process one-hot categorical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_one_hot(x):\n",
    "    \"\"\"\n",
    "    Takes a list/array/series and returns 1 for highest value and 0 for all \n",
    "    others\n",
    "    \n",
    "    \"\"\"\n",
    "    # Get argmax\n",
    "    highest = np.argmax(x)\n",
    "    # Set all values to zero\n",
    "    x *= 0.0\n",
    "    # Set argmax to one\n",
    "    x[highest] = 1.0\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to add nearest real data neighbour to synthetic data\n",
    "\n",
    "Find nearest neighbour in the real data set (based on Cartesian distance of standardised data).Find nearest neighbour in the real data set (based on Cartesian distance of standardised data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_distance_to_closest_real_data(\n",
    "    X_actual_train, X_actual_test, X_synthetic):\n",
    "\n",
    "    \"\"\"\n",
    "    Find nearest neighbour in the real data set (based on Cartesian distance of\n",
    "    standardised data).\n",
    "    \"\"\"\n",
    "    \n",
    "    # Standardise data (based on real training data)\n",
    "    X_train_std, X_synth_std = standardise_data(\n",
    "        X_actual_train, X_synthetic)\n",
    "    X_train_std, X_test_std = standardise_data(\n",
    "        X_actual_train, X_actual_test)\n",
    "\n",
    "    # Get all real X data (combine standardised training + test data)\n",
    "    X_real_std = np.concatenate([X_train_std, X_test_std], axis=0)\n",
    "\n",
    "    # Use ScitLearn neighbors.NearestNeighbors to get nearest neighbour    \n",
    "    nn = NearestNeighbors(n_neighbors=1, algorithm='auto').fit(X_real_std)\n",
    "    dists, idxs = nn.kneighbors(X_synth_std)\n",
    "\n",
    "    # Store in dictionary\n",
    "    nearest_neighbours = dict()\n",
    "    nearest_neighbours['distances'] = list(dists.flatten())\n",
    "    nearest_neighbours['ids'] = list(idxs.flatten())\n",
    "    \n",
    "    return nearest_neighbours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to remove identical or nearest neighbour points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_near_neighbours(df, frac_remove=0.0):\n",
    "    \"\"\"Remove identical or nearest neighbour points\"\"\"\n",
    "    \n",
    "    # Remove synethetic data points with identical real data points\n",
    "    # (use distance of <0.001 as effectively identical)\n",
    "    \n",
    "    identical = df['nn_distance'] < 0.001\n",
    "    mask = identical == False\n",
    "    df = df[mask]\n",
    "    \n",
    "    # Remove close neighbours    \n",
    "    df.sort_values('nn_distance', ascending=False, inplace=True)\n",
    "    number_to_keep = int(len(df) * (1 - frac_remove))\n",
    "    df = df.head(number_to_keep)\n",
    "    df = df.sample(frac=1.0)\n",
    "    \n",
    "    return df \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set lists of categorical (one-hot coded) and integer fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hot column lists\n",
    "\n",
    "col_list = [\n",
    "    'MoreEqual80y', \n",
    "    'S1Gender',\n",
    "    'S1Ethnicity',\n",
    "    'S1OnsetInHospital',\n",
    "    'S1OnsetTimeType',\n",
    "    'S1OnsetDateType',\n",
    "    'S1ArriveByAmbulance',\n",
    "    'S1AdmissionHour',\n",
    "    'S1AdmissionDay',\n",
    "    'S1AdmissionQuarter',\n",
    "    'S1AdmissionYear',\n",
    "    'CongestiveHeartFailure',\n",
    "    'Hypertension',\n",
    "    'AtrialFibrillation',\n",
    "    'Diabetes',\n",
    "    'StrokeTIA',\n",
    "    'AFAntiplatelet',\n",
    "    'AFAnticoagulent',\n",
    "    'S2NewAFDiagnosis',\n",
    "    'S2StrokeType',\n",
    "    'S2TIAInLastMonth']\n",
    "\n",
    "X_col_names = list(train_data[0])\n",
    "one_hot_cols = []\n",
    "for col in col_list:\n",
    "    one_hot = [x for x in X_col_names if x[0:len(col)] == col]\n",
    "    one_hot_cols.append(one_hot)\n",
    "    \n",
    "integer_cols = [\n",
    "    'S1AgeOnArrival',\n",
    "    'S2RankinBeforeStroke',\n",
    "    'Loc',\n",
    "    'LocQuestions',\n",
    "    'LocCommands',\n",
    "    'BestGaze',\n",
    "    'Visual',\n",
    "    'FacialPalsy',\n",
    "    'MotorArmLeft',\n",
    "    'MotorArmRight',\n",
    "    'MotorLegLeft',\n",
    "    'MotorLegRight',\n",
    "    'LimbAtaxia',\n",
    "    'Sensory',\n",
    "    'BestLanguage',\n",
    "    'Dysarthria',\n",
    "    'ExtinctionInattention',\n",
    "    'S2NihssArrival']\n",
    "\n",
    "# Get min and max for integers (will be used to clip synthetic data)\n",
    "\n",
    "integer_min_max = dict()\n",
    "for col in integer_cols:\n",
    "    col_min = int(train_data[0][col].min())\n",
    "    col_max = int(train_data[0][col].max())\n",
    "    integer_min_max[col] = (col_min, col_max)\n",
    "    \n",
    "# Manually clip age to 30 - 100 to avoid using extremes\n",
    "integer_min_max['S1AgeOnArrival'] = (30, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create synthetic data\n",
    "\n",
    "### Create synthetic data and process categorical (one-hot) and integer data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add distance measure to original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up list of K-fold synthetic data\n",
    "k_fold_synthetic_data_with_distance = []\n",
    "\n",
    "# Loop through k-folds\n",
    "num_k_folds=5\n",
    "for k_fold in range(num_k_folds):\n",
    "       \n",
    "    X_train = train_data[k_fold].drop(['S2Thrombolysis'], axis=1)\n",
    "    X_test = test_data[k_fold].drop(['S2Thrombolysis'], axis=1)\n",
    "    X_y_synthetic = k_fold_synthetic_data[k_fold]\n",
    "    \n",
    "    # Loop through stroke teams (calculate distance to patients in same unit)\n",
    "    groups = X_train.groupby('StrokeTeam')\n",
    "    \n",
    "    # Set up list for synthetic data for each stroke team\n",
    "    synthetic_dfs = []\n",
    "    \n",
    "    for index, group_df in groups:\n",
    "        \n",
    "        # Get training, test, and synethetic data for each k-fold\n",
    "        actual_train = group_df.drop(['StrokeTeam'], axis=1)\n",
    "        mask = X_test['StrokeTeam'] == index\n",
    "        actual_test = X_test[mask].drop(['StrokeTeam'], axis=1)\n",
    "        mask = X_y_synthetic['StrokeTeam'] == index\n",
    "        synthetic_X = X_y_synthetic[mask].drop(\n",
    "            ['StrokeTeam', 'S2Thrombolysis'], axis=1)\n",
    "        synthetic_Y = X_y_synthetic[mask]['S2Thrombolysis']\n",
    "        \n",
    "        # Get nearest neighbour distance and ID\n",
    "        nearest_neighbours = find_distance_to_closest_real_data(\n",
    "            actual_train, actual_test, synthetic_X)\n",
    "        \n",
    "        # Store data for unit in a dataframe\n",
    "        df = X_y_synthetic[mask]\n",
    "        df['nn_distance'] = nearest_neighbours['distances']\n",
    "        df['nn_id'] = nearest_neighbours['ids']\n",
    "        \n",
    "        # Append to list of unit dataframes\n",
    "        synthetic_dfs.append(df)\n",
    "        \n",
    "    # Concatenate results, shuffle and store\n",
    "    synthetic_data = pd.concat(synthetic_dfs)\n",
    "    synthetic_data = synthetic_data.sample(frac=1.0)\n",
    "    k_fold_synthetic_data_with_distance.append(synthetic_data)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove points with identical points or near neighbours in real data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove points with identical points or near neighbours in real data (these are examined by unit - two identical patients may exist if they are associated with different stroke units).~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_fold_synthetic_data_restricted = []\n",
    "\n",
    "for k_fold in range(num_k_folds):\n",
    "    \n",
    "    # Remove points with identical points or near neighbours in real data \n",
    "    restricted_data = remove_near_neighbours(\n",
    "        k_fold_synthetic_data_with_distance[k_fold], 0.1)\n",
    "    \n",
    "    # Sample from synthetic data to get same size/balance as the original data\n",
    "    original_data = pd.concat([train_data[k_fold], test_data[k_fold]], axis=0)                               \n",
    "    num_class_0 = np.sum(original_data['S2Thrombolysis'] == 0)\n",
    "    num_class_1 = np.sum(original_data['S2Thrombolysis'] == 1)\n",
    "    \n",
    "    mask = restricted_data['S2Thrombolysis'] == 0\n",
    "    synth_class_0 = restricted_data[mask].sample(num_class_0, replace=True)\n",
    "    mask = restricted_data['S2Thrombolysis'] == 1\n",
    "    synth_class_1 = restricted_data[mask].sample(num_class_1, replace=True)\n",
    "    \n",
    "    # Reconstruct into single dataframe and shuffle\n",
    "    df = pd.concat([synth_class_0, synth_class_1], axis=0)\n",
    "    df = df.sample(frac=1.0)\n",
    "    \n",
    "    k_fold_synthetic_data_restricted.append(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = list(train_data[0])\n",
    "for k_fold in range(num_k_folds):\n",
    "    # Save complete data\n",
    "    data = k_fold_synthetic_data_restricted[k_fold]\n",
    "    name = data_loc + f'synthetic_with_nn_2_{k_fold}.csv'\n",
    "    data.to_csv(name, index=False)\n",
    "    # Save restricted data without nearest neighbour info\n",
    "    data = k_fold_synthetic_data_restricted[k_fold]\n",
    "    data = data[cols]\n",
    "    name = data_loc + f'synthetic_2_{k_fold}.csv'\n",
    "    data.to_csv(name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
