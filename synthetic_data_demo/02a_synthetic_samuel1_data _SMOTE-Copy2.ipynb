{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating synthetic SAMUeL data  data with SMOTE\n",
    "\n",
    "## Description of SMOTE\n",
    "\n",
    "SMOTE stands for Synthetic Minority Oversampling Technique [1]. SMOTE is more commonly used to create additional data to enhance modelling fitting, especially when one or more classes have low prevalence in the data set. Hence the description of *oversampling*. \n",
    "\n",
    "SMOTE works by finding near-neighbor points in the original data, and creating new data points from interpolating between two near-neighbor points.\n",
    "\n",
    "Here we remove the real data used to create the synthetic data, leaving only the synthetic data. After generating synthetic data we remove any data points that, by chance, are identical to original real data points, and also remove 10% of points that are closest to the original data points. We measure 'closeness' by the Cartesian distance between standardised data values.\n",
    "\n",
    "![](./images/smote.png)\n",
    "\n",
    "*Demonstration of SMOTE method. (a) Data points with two features (shown on x and y axes) are represented. Points are colour-coded by class label. (b) A data point from a class is picked at random, shown by the black point, and then the closest neighbours of the same class are identified, as shown by yellow points. Here we show 3 closest neighbours, but the default in the SMOTE `Imbalanced-Learn` library is 6. One of those near-neighbour points is selected at random (shown by the second black point). A new data point, shown in red, is created at a random distance between the two selected data points.*\n",
    "\n",
    "### Handling integer, binary, and categorical data\n",
    "\n",
    "The standard SMOTE method generates floating point non-integer) values between data points. There are alternative ways of handing integer, binary, and categorical data using the SMOTE method. Here the methods we use are:\n",
    "\n",
    "* *Integer* values: Round the resulting synthetic data point value to the closest integer.\n",
    "\n",
    "* *Binary*: Code the value as 0 or 1, and round the resulting synthetic data point value to the closest integer (0 or 1).\n",
    "\n",
    "* *Categorical*: One-hot encode the categorical feature. Generate the synthetic data for each category value. Identify the category with the highest value and set to 1 while setting all others to 0.\n",
    "\n",
    "### Implementation with IMBLEARN\n",
    "\n",
    "Here use the implementation in the IMBLEARN IMBALANCED-LEARN [2] \n",
    "\n",
    "[1] Chawla, N.V., Bowyer, K.W., Hall, L.O., Kegelmeyer, W.P. “SMOTE: Synthetic minority over-sampling technique,” Journal of Artificial Intelligence Research, vol. 16, pp. 321-357, 2002.\n",
    "\n",
    "[2] Lemaitre, G., Nogueira, F. and Aridas, C. (2016), Imbalanced-learn: A Python Toolbox to Tackle the Curse of Imbalanced Datasets in Machine Learning. arXiv:1609.06570 (https://pypi.org/project/imbalanced-learn/, `pip install imbalanced-learn`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Import machine learning methods\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# Import package for SMOTE\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Turn warnings off to keep notebook clean\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set minimum allowable distance from synthetic to closest real data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_epsilon = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data\n",
    "\n",
    "Synthetic data is based on the training data of a train/test split so performance can then be tested against a test set not used to make synthetic data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loc = './../data/sam_1/kfold_5fold/'\n",
    "train_data = pd.read_csv(data_loc + 'train_2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to standardise data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardise_data(X_train, X_test):\n",
    "    \"\"\"\n",
    "    Converts all data to a similar scale.\n",
    "    Standardisation subtracts mean and divides by standard deviation\n",
    "    for each feature.\n",
    "    Standardised data will have a mena of 0 and standard deviation of 1.\n",
    "    The training data mean and standard deviation is used to standardise both\n",
    "    training and test set data.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialise a new scaling object for normalising input data\n",
    "    sc = StandardScaler() \n",
    "\n",
    "    # Set up the scaler just on the training set\n",
    "    sc.fit(X_train)\n",
    "\n",
    "    # Apply the scaler to the training and test sets\n",
    "    train_std=sc.transform(X_train)\n",
    "    test_std=sc.transform(X_test)\n",
    "    \n",
    "    return train_std, test_std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to calculate accuracy measures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to create raw synthetic data\n",
    "\n",
    "Data generated are floats; this will need processing for integer, binary, and categorical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_synthetic_data_smote(X, y, number_of_samples=[1000,1000]):\n",
    "    \"\"\"\n",
    "    Synthetic data generation for two classes.\n",
    "        \n",
    "    Inputs\n",
    "    ------\n",
    "    original_data: X, y numpy arrays (y should have label 0 and 1)\n",
    "    number_of_samples: number of samples to generate (list for y=0, y=1)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    X_synthetic: NumPy array\n",
    "    y_synthetic: NumPy array\n",
    "    \"\"\"\n",
    "       \n",
    "    # Count instances in each class\n",
    "    count_label_0 = np.sum(y==0)\n",
    "    count_label_1 = np.sum(y==1)\n",
    "    \n",
    "    # SMOTE requires final class counts; add current counts to required counts\n",
    "    n_class_0 = number_of_samples[0] + count_label_0\n",
    "    n_class_1 = number_of_samples[1] + count_label_1\n",
    "\n",
    "    # Get SMOTE points\n",
    "    X_resampled, y_resampled = SMOTE(\n",
    "        sampling_strategy = {0:n_class_0, 1:n_class_1}, n_jobs=-1\n",
    "        ).fit_resample(X, y)\n",
    "\n",
    "    # Get just the additional (synethetic) data points\n",
    "    X_synthetic = X_resampled[len(X):]\n",
    "    y_synthetic = y_resampled[len(y):]\n",
    "                                                                   \n",
    "    return X_synthetic, y_synthetic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to process one-hot categorical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_one_hot(x):\n",
    "    \"\"\"\n",
    "    Takes a list/array/series and returns 1 for highest value and 0 for all \n",
    "    others\n",
    "    \n",
    "    \"\"\"\n",
    "    # Get argmax\n",
    "    highest = np.argmax(x)\n",
    "    # Set all values to zero\n",
    "    x *= 0.0\n",
    "    # Set argmax to one\n",
    "    x[highest] = 1.0\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to add nearest real data neighbour to synthetic data\n",
    "\n",
    "Find nearest neighbour in the real data set (based on Cartesian distance of standardised data).Find nearest neighbour in the real data set (based on Cartesian distance of standardised data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_distance_to_closest_real_data(X_actual, X_synthetic):\n",
    "\n",
    "    \"\"\"\n",
    "    Find nearest neighbour in the real data set (based on Cartesian distance of\n",
    "    standardised data).\n",
    "    \"\"\"\n",
    "    \n",
    "    # Standardise data (based on real training data)\n",
    "    X_actual_std, X_synth_std = standardise_data(X_actual, X_synthetic)\n",
    "\n",
    "    # Use ScitLearn neighbors.NearestNeighbors to get nearest neighbour    \n",
    "    nn = NearestNeighbors(n_neighbors=1, algorithm='auto').fit(X_actual_std)\n",
    "    dists, idxs = nn.kneighbors(X_synth_std)\n",
    "\n",
    "    # Convert row idxs to index in original index in X_actual\n",
    "    real_ids = list(X_actual.iloc[idxs.flatten()].index)\n",
    "\n",
    "    # Store in dictionary\n",
    "    nearest_neighbours = dict()\n",
    "    nearest_neighbours['distances'] = list(dists.flatten())\n",
    "    nearest_neighbours['ids'] = real_ids\n",
    "    \n",
    "    return nearest_neighbours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to remove identical or nearest neighbour points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_near_neighbours(df, epsilon=0.0, frac_remove=0.0):\n",
    "    \"\"\"Remove identical or nearest neighbour points\"\"\"\n",
    "    \n",
    "    # Remove points within defined distance    \n",
    "    if epsilon > 0:\n",
    "        identical = df['nn_distance'] < epsilon\n",
    "        mask = identical == False\n",
    "        df = df[mask]\n",
    "    \n",
    "    # Remove a proportion (closest neighbours)\n",
    "    if frac_remove > 0:\n",
    "        df.sort_values('nn_distance', ascending=False, inplace=True)\n",
    "        number_to_keep = int(len(df) * (1 - frac_remove))\n",
    "        df = df.head(number_to_keep)\n",
    "        df = df.sample(frac=1.0)\n",
    "    \n",
    "    return df \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set lists of categorical (one-hot coded) and integer fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hot column lists\n",
    "\n",
    "col_list = [\n",
    "    'MoreEqual80y', \n",
    "    'S1Gender',\n",
    "    'S1Ethnicity',\n",
    "    'S1OnsetInHospital',\n",
    "    'S1OnsetTimeType',\n",
    "    'S1OnsetDateType',\n",
    "    'S1ArriveByAmbulance',\n",
    "    'S1AdmissionHour',\n",
    "    'S1AdmissionDay',\n",
    "    'S1AdmissionQuarter',\n",
    "    'S1AdmissionYear',\n",
    "    'CongestiveHeartFailure',\n",
    "    'Hypertension',\n",
    "    'AtrialFibrillation',\n",
    "    'Diabetes',\n",
    "    'StrokeTIA',\n",
    "    'AFAntiplatelet',\n",
    "    'AFAnticoagulentVitK',\n",
    "    'AFAnticoagulentDOAC',\n",
    "    'AFAnticoagulentHeparin',\n",
    "    'S2NewAFDiagnosis',\n",
    "    'S2StrokeType',\n",
    "    'S2TIAInLastMonth']\n",
    "\n",
    "X_col_names = list(train_data)\n",
    "one_hot_cols = []\n",
    "for col in col_list:\n",
    "    one_hot = [x for x in X_col_names if x[0:len(col)] == col]\n",
    "    one_hot_cols.append(one_hot)\n",
    "    \n",
    "integer_cols = [\n",
    "    'S2RankinBeforeStroke',\n",
    "    'Loc',\n",
    "    'LocQuestions',\n",
    "    'LocCommands',\n",
    "    'BestGaze',\n",
    "    'Visual',\n",
    "    'FacialPalsy',\n",
    "    'MotorArmLeft',\n",
    "    'MotorArmRight',\n",
    "    'MotorLegLeft',\n",
    "    'MotorLegRight',\n",
    "    'LimbAtaxia',\n",
    "    'Sensory',\n",
    "    'BestLanguage',\n",
    "    'Dysarthria',\n",
    "    'ExtinctionInattention',\n",
    "    'S2NihssArrival']\n",
    "\n",
    "# Get min and max for integers (will be used to clip synthetic data)\n",
    "\n",
    "integer_min_max = dict()\n",
    "for col in integer_cols:\n",
    "    col_min = int(train_data[col].min())\n",
    "    col_max = int(train_data[col].max())\n",
    "    integer_min_max[col] = (col_min, col_max)\n",
    "    \n",
    "# Manually clip age to 30 - 100 to avoid using extremes\n",
    "integer_min_max['S1AgeOnArrival'] = (30, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create synthetic data\n",
    "\n",
    "### Create synthetic data and process categorical (one-hot) and integer data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synthesising 132 out of 132 teams"
     ]
    }
   ],
   "source": [
    "# Set up list for synthetic data for each stroke team\n",
    "synthetic_dfs = []\n",
    "\n",
    "# Loop through stroke teams\n",
    "groups = train_data.groupby('StrokeTeam')\n",
    "count = 0 \n",
    "\n",
    "for index, group_df in groups:\n",
    "\n",
    "    # Report progress\n",
    "    count += 1\n",
    "    print (f'\\rSynthesising {count} out of {len(groups)} teams', end='')\n",
    "\n",
    "    # Split data in X and y\n",
    "    X = group_df.drop(['S2Thrombolysis'], axis=1)\n",
    "    X.drop('StrokeTeam', inplace = True, axis=1)\n",
    "    y = group_df['S2Thrombolysis']\n",
    "\n",
    "    X_col_names = list(X)\n",
    "    X = X.values\n",
    "    y = y.values\n",
    "\n",
    "    # Count number in each class\n",
    "    count_label_0 = np.sum(y == 0)\n",
    "    count_label_1 = np.sum(y == 1)\n",
    "\n",
    "    # Will make intial SMOTE data to be 2 x size of data to allow point removal\n",
    "    n_class_0 = int(2 * count_label_0)\n",
    "    n_class_1 = int(2 * count_label_1)\n",
    "\n",
    "    # Generate raw synthetic data\n",
    "\n",
    "    X_synthetic, y_synthetic = make_synthetic_data_smote(\n",
    "        X, y, [n_class_0, n_class_1])\n",
    "\n",
    "    # Reconstruct dataframe\n",
    "    df = pd.DataFrame(X_synthetic, columns=X_col_names)\n",
    "    df['S2Thrombolysis'] = y_synthetic\n",
    "    df['StrokeTeam'] = index\n",
    "\n",
    "    # Make one hot as necessary\n",
    "    for one_hot_list in one_hot_cols:\n",
    "        for row_index, row in df.iterrows():\n",
    "            x = row[one_hot_list]\n",
    "            x_one_hot = make_one_hot(x)\n",
    "            row[x_one_hot.index] = x_one_hot.values\n",
    "            df.loc[row_index] = row\n",
    "\n",
    "    # Round and clip integer columns\n",
    "    for col in integer_cols:\n",
    "        df[col] = df[col].round(0)\n",
    "        # Clip\n",
    "        df[col] = np.clip(\n",
    "            df[col], integer_min_max[col][0], integer_min_max[col][1])\n",
    "\n",
    "    # Add to list\n",
    "    synthetic_dfs.append(df)\n",
    "\n",
    "# Concatenate results, shuffle and store\n",
    "synthetic_data = pd.concat(synthetic_dfs)\n",
    "synthetic_data = synthetic_data.sample(frac=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add distance measure to original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "X_train = train_data.drop(['S2Thrombolysis'], axis=1)\n",
    "\n",
    "# Loop through stroke teams (calculate distance to patients in same unit)\n",
    "groups = X_train.groupby('StrokeTeam')\n",
    "\n",
    "# Set up list for synthetic data for each stroke team\n",
    "synthetic_dfs = []\n",
    "\n",
    "for index, group_df in groups:\n",
    "\n",
    "    # Get training, test, and synthetic data for each k-fold\n",
    "    actual_train = group_df.drop(['StrokeTeam'], axis=1)\n",
    "    mask = synthetic_data['StrokeTeam'] == index\n",
    "    synthetic_X = synthetic_data[mask].drop(\n",
    "        ['StrokeTeam', 'S2Thrombolysis'], axis=1)\n",
    "    synthetic_Y = synthetic_data[mask]['S2Thrombolysis']\n",
    "\n",
    "    # Get nearest neighbour distance and ID\n",
    "    nearest_neighbours = find_distance_to_closest_real_data(\n",
    "        actual_train, synthetic_X)\n",
    "\n",
    "    # Store data for unit in a dataframe\n",
    "    df = synthetic_data[mask]\n",
    "    df['nn_distance'] = nearest_neighbours['distances']\n",
    "    df['nn_id'] = nearest_neighbours['ids']\n",
    "\n",
    "    # Append to list of unit dataframes\n",
    "    synthetic_dfs.append(df)\n",
    "\n",
    "# Concatenate results, shuffle and store\n",
    "synthetic_data = pd.concat(synthetic_dfs)\n",
    "synthetic_data = synthetic_data.sample(frac=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove points with identical points or near neighbours in real data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove points with identical points or near neighbours in real data (these are examined by unit - two identical patients may exist if they are associated with different stroke units).~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Remove points within  \n",
    "restricted_data = remove_near_neighbours(synthetic_data, min_epsilon)\n",
    "\n",
    "# Sample from synthetic data to get same size/balance as the original data\n",
    "num_class_0 = np.sum(train_data['S2Thrombolysis'] == 0)\n",
    "num_class_1 = np.sum(train_data['S2Thrombolysis'] == 1)\n",
    "\n",
    "mask = restricted_data['S2Thrombolysis'] == 0\n",
    "samples_available = sum(mask)\n",
    "samples_without_bootstrap = min(num_class_0, samples_available)\n",
    "samples_with_bootstrap = max(0, num_class_0 - samples_without_bootstrap)\n",
    "synth_class_0 = pd.concat([\n",
    "    restricted_data[mask].sample(samples_without_bootstrap),\n",
    "    restricted_data[mask].sample(samples_with_bootstrap, replace=True)])\n",
    "\n",
    "mask = restricted_data['S2Thrombolysis'] == 1\n",
    "samples_available = sum(mask)\n",
    "samples_without_bootstrap = min(num_class_1, samples_available)\n",
    "samples_with_bootstrap = max(0, num_class_1 - samples_without_bootstrap)\n",
    "synth_class_1 = pd.concat([\n",
    "    restricted_data[mask].sample(samples_without_bootstrap),\n",
    "    restricted_data[mask].sample(samples_with_bootstrap, replace=True)])\n",
    "\n",
    "# Reconstruct into single dataframe and shuffle\n",
    "synthetic_data_restricted = pd.concat([synth_class_0, synth_class_1], axis=0)\n",
    "synthetic_data_restricted = synthetic_data_restricted.sample(frac=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Save with same columns and order as orginal data\n",
    "cols = list(train_data)\n",
    "synthetic_data_restricted = synthetic_data_restricted[cols]\n",
    "name = data_loc + f'synthetic_2.csv'\n",
    "synthetic_data_restricted.to_csv(name, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
